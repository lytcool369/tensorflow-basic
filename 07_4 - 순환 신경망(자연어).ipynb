{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.32 조선왕조실록 데이터 파일 다운로드\n",
    "import tensorflow as tf\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 26265493 characters\n",
      "\n",
      "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
      "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n"
     ]
    }
   ],
   "source": [
    "# 7.33 데이터 로드 및 확인\n",
    "# 데이터를 메모리에 불러옵니다. 인코딩 형식으로 utf-8,을 지정해야 합니다.\n",
    "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 텍스트가 총 몇 자인지 확인합니다.\n",
    "print('Length of text: {} characters'.format(len(train_text)))\n",
    "print()\n",
    "\n",
    "# 처음 100자를 확인해 봅니다.\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n"
     ]
    }
   ],
   "source": [
    "# 7.34 훈련 데이터 입력 정제\n",
    "import re\n",
    "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    \n",
    "    return string\n",
    "\n",
    "train_text = train_text.split('\\n')\n",
    "train_text = [clean_str(sentence) for sentence in train_text]\n",
    "train_text_X = []\n",
    "\n",
    "for sentence in train_text:\n",
    "    train_text_X.extend(sentence.split(' '))\n",
    "    train_text_X.append('\\n')\n",
    "    \n",
    "train_text_X = [word for word in train_text_X if word != '']\n",
    "\n",
    "print(train_text_X[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332640 unique words\n",
      "{\n",
      "  '\\n':   0,\n",
      "  '!' :   1,\n",
      "  ',' :   2,\n",
      "  '000명으로':   3,\n",
      "  '001':   4,\n",
      "  '002':   5,\n",
      "  '003':   6,\n",
      "  '004':   7,\n",
      "  '005':   8,\n",
      "  '006':   9,\n",
      "  ...\n",
      "}\n",
      "index of UNK: 332639\n"
     ]
    }
   ],
   "source": [
    "# 7.35 단어 토큰화\n",
    "# 단어의 set을 만듭니다.\n",
    "import numpy as np\n",
    "\n",
    "vocab = sorted(set(train_text_X))\n",
    "vocab.append('UNK')\n",
    "print('{} unique words'.format(len(vocab)))\n",
    "\n",
    "# vocab list를 숫자로 매핑하고, 반대도 실행합니다.\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
    "\n",
    "# word2idx 의 일부를 알아보기 쉽게 출력해 봅니다.\n",
    "print('{')\n",
    "\n",
    "for word,_ in zip(word2idx, range(10)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
    "print('  ...\\n}')\n",
    "\n",
    "print('index of UNK: {}'.format(word2idx['UNK']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413]\n"
     ]
    }
   ],
   "source": [
    "# 7.36 토큰 데이터 확인\n",
    "print(train_text_X[:20])\n",
    "print(text_as_int[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
      " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',' '휘']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
      " 224182 164549 230248 210912      2 330313]\n"
     ]
    }
   ],
   "source": [
    "# 7.37 기본 데이터 세트 만들기\n",
    "seq_length = 25\n",
    "examples_per_epoch = len(text_as_int) // seq_length\n",
    "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sentence_dataset = sentence_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for item in sentence_dataset.take(1):\n",
    "    print(idx2word[item.numpy()])\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
      " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
      " 224182 164549 230248 210912      2]\n",
      "휘\n",
      "330313\n"
     ]
    }
   ],
   "source": [
    "# 7.38 학습 데이터세트 만들기\n",
    "def split_input_target(chunk):\n",
    "    return [chunk[:-1], chunk[-1]]\n",
    "\n",
    "train_dataset = sentence_dataset.map(split_input_target)\n",
    "\n",
    "for x, y in train_dataset.take(1):\n",
    "    print(idx2word[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx2word[y.numpy()])\n",
    "    print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터세트 shuffle, batch 설정\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 100)           33264000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 25, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 332640)            33596640  \n",
      "=================================================================\n",
      "Total params: 67,021,440\n",
      "Trainable params: 67,021,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7.40 단어 단위 생성 모델 정의\n",
    "total_words = len(vocab)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(units=100),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 2135 steps\n",
      "Epoch 1/50\n",
      "\n",
      " 태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "2135/2135 - 692s - loss: 9.1674 - accuracy: 0.0731\n",
      "Epoch 2/50\n",
      "2135/2135 - 684s - loss: 8.0491 - accuracy: 0.0797\n",
      "Epoch 3/50\n",
      "2135/2135 - 676s - loss: 7.6663 - accuracy: 0.0889\n",
      "Epoch 4/50\n",
      "2135/2135 - 677s - loss: 7.2867 - accuracy: 0.1069\n",
      "Epoch 5/50\n",
      "2135/2135 - 675s - loss: 6.8171 - accuracy: 0.1234\n",
      "Epoch 6/50\n",
      "\n",
      " 태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  , 그 계하기를 , \n",
      " \n",
      " \n",
      " \n",
      " 이 말하기를 , \n",
      " \n",
      " 하니 , 그대로 따랐다 임금이 말하기를 , \n",
      " 내가 그 그 짓게 하고 , 그 그 굽혀 그 굽혀 그 인도하여 인도하여 영좌 것이 없으니 , 전하는 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여 인도하여\n",
      "\n",
      "2135/2135 - 676s - loss: 6.2984 - accuracy: 0.1401\n",
      "Epoch 7/50\n"
     ]
    }
   ],
   "source": [
    "# 7.41 단어 단위 생성 모델 학습\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    if epoch % 5 != 0 and epoch != 49:\n",
    "        return\n",
    "    \n",
    "    test_sentence = train_text[0]\n",
    "    next_words = 100\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
    "        \n",
    "        output_idx = model.predict_classes(test_text_X)\n",
    "        test_sentence += ' ' + idx2word[output_idx[0]]\n",
    "        \n",
    "    print()\n",
    "    print(test_sentence)\n",
    "    print()\n",
    "    \n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
    "\n",
    "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
